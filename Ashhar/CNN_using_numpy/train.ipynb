{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN From Scratch In Python! \n",
    "This notebook is used for training of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import mnist\n",
    "from conv import ConvolutionLayer\n",
    "from maxpool import MaxPoolingLayer\n",
    "from activations import Softmax\n",
    "from training import CNN_train, CNN_test, CNN_predict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieveing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.getData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the layers of our CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    ConvolutionLayer(1, 3),\n",
    "    MaxPoolingLayer(2),\n",
    "    Softmax(13*13*1, 10)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN model\n",
    "Now that we have defined the layers in our model, we will now train the models on the test data we got from the test train split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 23.146727054002234, accuracy 102\n",
      "Step 2001. For the last 1000 steps: average loss 23.167831809034066, accuracy 101\n",
      "Step 3001. For the last 1000 steps: average loss 23.153170402735142, accuracy 101\n",
      "Step 4001. For the last 1000 steps: average loss 23.17185051997663, accuracy 85\n",
      "Step 5001. For the last 1000 steps: average loss 23.146806974462482, accuracy 104\n",
      "Step 6001. For the last 1000 steps: average loss 23.12797803677623, accuracy 95\n",
      "Step 7001. For the last 1000 steps: average loss 23.171098676221096, accuracy 85\n",
      "Step 8001. For the last 1000 steps: average loss 23.107260807802035, accuracy 100\n",
      "Step 9001. For the last 1000 steps: average loss 22.996907853822485, accuracy 120\n",
      "Epoch 2 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 22.839638964715128, accuracy 156\n",
      "Step 2001. For the last 1000 steps: average loss 22.577819880611628, accuracy 179\n",
      "Step 3001. For the last 1000 steps: average loss 21.986693096351164, accuracy 325\n",
      "Step 4001. For the last 1000 steps: average loss 21.17156936247032, accuracy 461\n",
      "Step 5001. For the last 1000 steps: average loss 19.568860889242536, accuracy 574\n",
      "Step 6001. For the last 1000 steps: average loss 17.456948288662865, accuracy 648\n",
      "Step 7001. For the last 1000 steps: average loss 14.818368433312235, accuracy 742\n",
      "Step 8001. For the last 1000 steps: average loss 13.378245160533, accuracy 737\n",
      "Step 9001. For the last 1000 steps: average loss 11.028623559221915, accuracy 754\n",
      "Epoch 3 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 8.336598774303845, accuracy 806\n",
      "Step 2001. For the last 1000 steps: average loss 7.405765049669476, accuracy 820\n",
      "Step 3001. For the last 1000 steps: average loss 6.224552488233413, accuracy 845\n",
      "Step 4001. For the last 1000 steps: average loss 6.02343217172268, accuracy 873\n",
      "Step 5001. For the last 1000 steps: average loss 5.658625415179326, accuracy 866\n",
      "Step 6001. For the last 1000 steps: average loss 5.353378449381917, accuracy 860\n",
      "Step 7001. For the last 1000 steps: average loss 4.694732462724133, accuracy 878\n",
      "Step 8001. For the last 1000 steps: average loss 5.708433132540937, accuracy 848\n",
      "Step 9001. For the last 1000 steps: average loss 5.585858256426268, accuracy 850\n",
      "Epoch 4 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 4.909699704480753, accuracy 869\n",
      "Step 2001. For the last 1000 steps: average loss 4.570066761546726, accuracy 868\n",
      "Step 3001. For the last 1000 steps: average loss 3.9406674877400603, accuracy 886\n",
      "Step 4001. For the last 1000 steps: average loss 3.9476120756006448, accuracy 905\n",
      "Step 5001. For the last 1000 steps: average loss 4.1221349696144145, accuracy 887\n",
      "Step 6001. For the last 1000 steps: average loss 4.022679387549978, accuracy 886\n",
      "Step 7001. For the last 1000 steps: average loss 3.547061863295935, accuracy 894\n",
      "Step 8001. For the last 1000 steps: average loss 4.5566685257004815, accuracy 879\n",
      "Step 9001. For the last 1000 steps: average loss 4.76423448611665, accuracy 866\n",
      "Epoch 5 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 4.226072731227584, accuracy 880\n",
      "Step 2001. For the last 1000 steps: average loss 3.936606450527374, accuracy 886\n",
      "Step 3001. For the last 1000 steps: average loss 3.351152928739403, accuracy 904\n",
      "Step 4001. For the last 1000 steps: average loss 3.3440765713349565, accuracy 916\n",
      "Step 5001. For the last 1000 steps: average loss 3.6581368632420475, accuracy 891\n",
      "Step 6001. For the last 1000 steps: average loss 3.5647981907333195, accuracy 899\n",
      "Step 7001. For the last 1000 steps: average loss 3.143239875644188, accuracy 899\n",
      "Step 8001. For the last 1000 steps: average loss 4.109147560440103, accuracy 884\n",
      "Step 9001. For the last 1000 steps: average loss 4.431664636165743, accuracy 879\n",
      "Epoch 6 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 3.9248500674652966, accuracy 886\n",
      "Step 2001. For the last 1000 steps: average loss 3.660152810392501, accuracy 895\n",
      "Step 3001. For the last 1000 steps: average loss 3.0696144716033733, accuracy 912\n",
      "Step 4001. For the last 1000 steps: average loss 3.0498166199259695, accuracy 922\n",
      "Step 5001. For the last 1000 steps: average loss 3.4181835827804368, accuracy 899\n",
      "Step 6001. For the last 1000 steps: average loss 3.3171931637158583, accuracy 909\n",
      "Step 7001. For the last 1000 steps: average loss 2.933156029128061, accuracy 905\n",
      "Step 8001. For the last 1000 steps: average loss 3.861449229227659, accuracy 886\n",
      "Step 9001. For the last 1000 steps: average loss 4.238818801655779, accuracy 883\n",
      "Epoch 7 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 3.747102923925541, accuracy 893\n",
      "Step 2001. For the last 1000 steps: average loss 3.500965671119699, accuracy 898\n",
      "Step 3001. For the last 1000 steps: average loss 2.8964383910204305, accuracy 913\n",
      "Step 4001. For the last 1000 steps: average loss 2.8702400499820304, accuracy 924\n",
      "Step 5001. For the last 1000 steps: average loss 3.262120261789479, accuracy 903\n",
      "Step 6001. For the last 1000 steps: average loss 3.155888976476171, accuracy 914\n",
      "Step 7001. For the last 1000 steps: average loss 2.8030962925909875, accuracy 910\n",
      "Step 8001. For the last 1000 steps: average loss 3.69812559086779, accuracy 887\n",
      "Step 9001. For the last 1000 steps: average loss 4.105393473267291, accuracy 884\n",
      "Epoch 8 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 3.625271006198184, accuracy 900\n",
      "Step 2001. For the last 1000 steps: average loss 3.3946647537897183, accuracy 901\n",
      "Step 3001. For the last 1000 steps: average loss 2.7752651023309287, accuracy 917\n",
      "Step 4001. For the last 1000 steps: average loss 2.7459141163210883, accuracy 926\n",
      "Step 5001. For the last 1000 steps: average loss 3.1485043103544377, accuracy 909\n",
      "Step 6001. For the last 1000 steps: average loss 3.0402596152911823, accuracy 917\n",
      "Step 7001. For the last 1000 steps: average loss 2.7138494109089, accuracy 915\n",
      "Step 8001. For the last 1000 steps: average loss 3.578843124959908, accuracy 891\n",
      "Step 9001. For the last 1000 steps: average loss 4.003468228497237, accuracy 888\n",
      "Epoch 9 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 3.533827923396146, accuracy 901\n",
      "Step 2001. For the last 1000 steps: average loss 3.3166907968980275, accuracy 902\n",
      "Step 3001. For the last 1000 steps: average loss 2.683806605763988, accuracy 920\n",
      "Step 4001. For the last 1000 steps: average loss 2.652655598870348, accuracy 928\n",
      "Step 5001. For the last 1000 steps: average loss 3.0603696578854964, accuracy 912\n",
      "Step 6001. For the last 1000 steps: average loss 2.952529596157378, accuracy 922\n",
      "Step 7001. For the last 1000 steps: average loss 2.6480777457119045, accuracy 918\n",
      "Step 8001. For the last 1000 steps: average loss 3.4859821855018187, accuracy 891\n",
      "Step 9001. For the last 1000 steps: average loss 3.920785129839977, accuracy 892\n",
      "Epoch 10 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 3.4609614214805635, accuracy 903\n",
      "Step 2001. For the last 1000 steps: average loss 3.25570421909013, accuracy 905\n",
      "Step 3001. For the last 1000 steps: average loss 2.6113171466757574, accuracy 923\n",
      "Step 4001. For the last 1000 steps: average loss 2.5788495368164894, accuracy 930\n",
      "Step 5001. For the last 1000 steps: average loss 2.9891801380372613, accuracy 916\n",
      "Step 6001. For the last 1000 steps: average loss 2.8833453472337935, accuracy 923\n",
      "Step 7001. For the last 1000 steps: average loss 2.5969117302956004, accuracy 921\n",
      "Step 8001. For the last 1000 steps: average loss 3.4106239099584075, accuracy 894\n",
      "Step 9001. For the last 1000 steps: average loss 3.851037336042, accuracy 896\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    print('Epoch {} ->'.format(epoch+1))\n",
    "    # Training the CNN\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    for i, (image, label) in enumerate(zip(X_train[0:10000], Y_train[0:10000])):\n",
    "      if i % 1000 == 0: # Every 1000 examples\n",
    "        print(\"Step {}. For the last 1000 steps: average loss {}, accuracy {}\".format(i+1, loss/100, accuracy))\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "      loss_1, accuracy_1 = CNN_train(image, label, layers)\n",
    "      loss += loss_1\n",
    "      accuracy += accuracy_1\n",
    "      # break the for loop if the models loss is less than 0.5\n",
    "      if(loss == 0.05):\n",
    "        print(\"Step {}. For the last 1000 steps: average loss {}, accuracy {}\".format(i+1, loss/100, accuracy))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our model\n",
    "Here we will test how our CNN model performs against the test data from the test train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 3.6151135831927435, accuracy 887\n",
      "Step 2001. For the last 1000 steps: average loss 4.50354909748911, accuracy 863\n",
      "Step 3001. For the last 1000 steps: average loss 4.336396146384858, accuracy 876\n",
      "Step 4001. For the last 1000 steps: average loss 4.145799733869785, accuracy 882\n",
      "Step 5001. For the last 1000 steps: average loss 3.951127503596536, accuracy 894\n",
      "Step 6001. For the last 1000 steps: average loss 2.2303354536766933, accuracy 933\n",
      "Step 7001. For the last 1000 steps: average loss 3.08811191838809, accuracy 910\n",
      "Step 8001. For the last 1000 steps: average loss 2.3585601279678117, accuracy 932\n",
      "Step 9001. For the last 1000 steps: average loss 1.271532498637427, accuracy 967\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "accuracy = 0\n",
    "for i, (image, label) in enumerate(zip(X_test[0:10000], Y_test[0:10000])):\n",
    "  if i % 1000 == 0: # Every 1000 examples\n",
    "    print(\"Step {}. For the last 1000 steps: average loss {}, accuracy {}\".format(i+1, loss/100, accuracy))\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "  loss_1, accuracy_1 = CNN_test(image, label, layers)\n",
    "  loss += loss_1\n",
    "  accuracy += accuracy_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x216f57b7250>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_img = X_test[600]\n",
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The models prediction: 6\n"
     ]
    }
   ],
   "source": [
    "cnn_prediction = CNN_predict(test_img, layers)\n",
    "print(\"The models prediction: \" + str(cnn_prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
