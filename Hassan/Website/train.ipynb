{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN From Scratch In Python! \n",
    "This notebook is used for training of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 19:57:34.255970: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-20 19:57:38.000872: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-20 19:57:38.000927: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-20 19:57:38.013452: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-20 19:57:39.562766: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-20 19:57:49.878886: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from dataset import mnist\n",
    "from conv import ConvolutionLayer\n",
    "from maxpool import MaxPoolingLayer\n",
    "from activations import Softmax\n",
    "from training import CNN_train, CNN_test, CNN_predict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieveing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.getData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the layers of our CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    ConvolutionLayer(1, 3),\n",
    "    MaxPoolingLayer(2),\n",
    "    Softmax(13*13*1, 10)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN model\n",
    "Now that we have defined the layers in our model, we will now train the models on the test data we got from the test train split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 23.146855918487717, accuracy 102\n",
      "Step 2001. For the last 1000 steps: average loss 23.167981829856814, accuracy 101\n",
      "Step 3001. For the last 1000 steps: average loss 23.153341325935934, accuracy 101\n",
      "Step 4001. For the last 1000 steps: average loss 23.17212839994585, accuracy 85\n",
      "Step 5001. For the last 1000 steps: average loss 23.147394166135943, accuracy 104\n",
      "Step 6001. For the last 1000 steps: average loss 23.12908798929621, accuracy 95\n",
      "Step 7001. For the last 1000 steps: average loss 23.17348891303821, accuracy 85\n",
      "Step 8001. For the last 1000 steps: average loss 23.111007486729576, accuracy 100\n",
      "Step 9001. For the last 1000 steps: average loss 23.00486022894813, accuracy 118\n",
      "Epoch 2 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 22.867254529876153, accuracy 149\n",
      "Step 2001. For the last 1000 steps: average loss 22.629262817434252, accuracy 165\n",
      "Step 3001. For the last 1000 steps: average loss 22.084495969475178, accuracy 306\n",
      "Step 4001. For the last 1000 steps: average loss 21.32943951951324, accuracy 441\n",
      "Step 5001. For the last 1000 steps: average loss 19.823160180542562, accuracy 560\n",
      "Step 6001. For the last 1000 steps: average loss 17.786087872896456, accuracy 637\n",
      "Step 7001. For the last 1000 steps: average loss 15.186722697945436, accuracy 735\n",
      "Step 8001. For the last 1000 steps: average loss 13.677345101987042, accuracy 730\n",
      "Step 9001. For the last 1000 steps: average loss 11.266424733609426, accuracy 748\n",
      "Epoch 3 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 8.46680464152186, accuracy 807\n",
      "Step 2001. For the last 1000 steps: average loss 7.501953134555061, accuracy 818\n",
      "Step 3001. For the last 1000 steps: average loss 6.296616662208323, accuracy 843\n",
      "Step 4001. For the last 1000 steps: average loss 6.08068782987957, accuracy 873\n",
      "Step 5001. For the last 1000 steps: average loss 5.701646627682191, accuracy 864\n",
      "Step 6001. For the last 1000 steps: average loss 5.389411013575987, accuracy 860\n",
      "Step 7001. For the last 1000 steps: average loss 4.724719428083358, accuracy 877\n",
      "Step 8001. For the last 1000 steps: average loss 5.735575498205225, accuracy 847\n",
      "Step 9001. For the last 1000 steps: average loss 5.604019592979529, accuracy 851\n",
      "Epoch 4 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 4.923190844277513, accuracy 868\n",
      "Step 2001. For the last 1000 steps: average loss 4.581570815609755, accuracy 868\n",
      "Step 3001. For the last 1000 steps: average loss 3.9532895947588362, accuracy 886\n",
      "Step 4001. For the last 1000 steps: average loss 3.9572562486372207, accuracy 904\n",
      "Step 5001. For the last 1000 steps: average loss 4.130515784695401, accuracy 887\n",
      "Step 6001. For the last 1000 steps: average loss 4.03254325736477, accuracy 885\n",
      "Step 7001. For the last 1000 steps: average loss 3.5548265630713485, accuracy 894\n",
      "Step 8001. For the last 1000 steps: average loss 4.565672262311071, accuracy 879\n",
      "Step 9001. For the last 1000 steps: average loss 4.769725967790087, accuracy 866\n",
      "Epoch 5 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 4.230239425946425, accuracy 881\n",
      "Step 2001. For the last 1000 steps: average loss 3.9397253838760924, accuracy 887\n",
      "Step 3001. For the last 1000 steps: average loss 3.356989048099656, accuracy 905\n",
      "Step 4001. For the last 1000 steps: average loss 3.347686717491468, accuracy 917\n",
      "Step 5001. For the last 1000 steps: average loss 3.6615067078768946, accuracy 891\n",
      "Step 6001. For the last 1000 steps: average loss 3.5706289595091647, accuracy 899\n",
      "Step 7001. For the last 1000 steps: average loss 3.1466740266758024, accuracy 899\n",
      "Step 8001. For the last 1000 steps: average loss 4.114231909744568, accuracy 884\n",
      "Step 9001. For the last 1000 steps: average loss 4.434294426758407, accuracy 880\n",
      "Epoch 6 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 3.926554034301741, accuracy 886\n",
      "Step 2001. For the last 1000 steps: average loss 3.6610118110947547, accuracy 895\n",
      "Step 3001. For the last 1000 steps: average loss 3.073440313935117, accuracy 912\n",
      "Step 4001. For the last 1000 steps: average loss 3.051698738456242, accuracy 922\n",
      "Step 5001. For the last 1000 steps: average loss 3.4199332221069283, accuracy 898\n",
      "Step 6001. For the last 1000 steps: average loss 3.321656535363552, accuracy 909\n",
      "Step 7001. For the last 1000 steps: average loss 2.9348663624163844, accuracy 906\n",
      "Step 8001. For the last 1000 steps: average loss 3.865027187903572, accuracy 886\n",
      "Step 9001. For the last 1000 steps: average loss 4.240257980232112, accuracy 884\n",
      "Epoch 7 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 3.7477522844612903, accuracy 892\n",
      "Step 2001. For the last 1000 steps: average loss 3.5008724556216686, accuracy 898\n",
      "Step 3001. For the last 1000 steps: average loss 2.899334796526672, accuracy 913\n",
      "Step 4001. For the last 1000 steps: average loss 2.8714075539132113, accuracy 925\n",
      "Step 5001. For the last 1000 steps: average loss 3.2631037265119343, accuracy 903\n",
      "Step 6001. For the last 1000 steps: average loss 3.1596441212468114, accuracy 914\n",
      "Step 7001. For the last 1000 steps: average loss 2.8039121891896235, accuracy 911\n",
      "Step 8001. For the last 1000 steps: average loss 3.700954000664658, accuracy 888\n",
      "Step 9001. For the last 1000 steps: average loss 4.106148598893765, accuracy 884\n",
      "Epoch 8 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 3.6253363739008093, accuracy 900\n",
      "Step 2001. For the last 1000 steps: average loss 3.394064744065804, accuracy 901\n",
      "Step 3001. For the last 1000 steps: average loss 2.777607578254932, accuracy 917\n",
      "Step 4001. For the last 1000 steps: average loss 2.7466676893701414, accuracy 926\n",
      "Step 5001. For the last 1000 steps: average loss 3.149029286241092, accuracy 908\n",
      "Step 6001. For the last 1000 steps: average loss 3.043547596735489, accuracy 915\n",
      "Step 7001. For the last 1000 steps: average loss 2.7141572325818255, accuracy 915\n",
      "Step 8001. For the last 1000 steps: average loss 3.5812213561892388, accuracy 891\n",
      "Step 9001. For the last 1000 steps: average loss 4.00377050707226, accuracy 888\n",
      "Epoch 9 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 3.5335131791491103, accuracy 902\n",
      "Step 2001. For the last 1000 steps: average loss 3.315778435042522, accuracy 902\n",
      "Step 3001. For the last 1000 steps: average loss 2.68577187819271, accuracy 920\n",
      "Step 4001. For the last 1000 steps: average loss 2.65313092038475, accuracy 929\n",
      "Step 5001. For the last 1000 steps: average loss 3.060615266293496, accuracy 912\n",
      "Step 6001. For the last 1000 steps: average loss 2.955475180732589, accuracy 922\n",
      "Step 7001. For the last 1000 steps: average loss 2.6480643578422423, accuracy 919\n",
      "Step 8001. For the last 1000 steps: average loss 3.4880572445268014, accuracy 891\n",
      "Step 9001. For the last 1000 steps: average loss 3.920759846295301, accuracy 892\n",
      "Epoch 10 ->\n",
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 3.4603814826047987, accuracy 904\n",
      "Step 2001. For the last 1000 steps: average loss 3.2545806452024904, accuracy 906\n",
      "Step 3001. For the last 1000 steps: average loss 2.6129916444127197, accuracy 923\n",
      "Step 4001. For the last 1000 steps: average loss 2.5791081336670207, accuracy 931\n",
      "Step 5001. For the last 1000 steps: average loss 2.9892359009343794, accuracy 916\n",
      "Step 6001. For the last 1000 steps: average loss 2.8860184351534723, accuracy 923\n",
      "Step 7001. For the last 1000 steps: average loss 2.596682665460967, accuracy 922\n",
      "Step 8001. For the last 1000 steps: average loss 3.4124835371209805, accuracy 893\n",
      "Step 9001. For the last 1000 steps: average loss 3.8507863261814204, accuracy 896\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    print('Epoch {} ->'.format(epoch+1))\n",
    "    # Training the CNN\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    for i, (image, label) in enumerate(zip(X_train[0:10000], Y_train[0:10000])):\n",
    "      if i % 1000 == 0: # Every 1000 examples\n",
    "        print(\"Step {}. For the last 1000 steps: average loss {}, accuracy {}\".format(i+1, loss/100, accuracy))\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "      loss_1, accuracy_1 = CNN_train(image, label, layers)\n",
    "      loss += loss_1\n",
    "      accuracy += accuracy_1\n",
    "      # break the for loop if the models loss is less than 0.5\n",
    "      if(loss == 0.05):\n",
    "        print(\"Step {}. For the last 1000 steps: average loss {}, accuracy {}\".format(i+1, loss/100, accuracy))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our model\n",
    "Here we will test how our CNN model performs against the test data from the test train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1. For the last 1000 steps: average loss 0.0, accuracy 0\n",
      "Step 1001. For the last 1000 steps: average loss 3.6151135831927435, accuracy 887\n",
      "Step 2001. For the last 1000 steps: average loss 4.50354909748911, accuracy 863\n",
      "Step 3001. For the last 1000 steps: average loss 4.336396146384858, accuracy 876\n",
      "Step 4001. For the last 1000 steps: average loss 4.145799733869785, accuracy 882\n",
      "Step 5001. For the last 1000 steps: average loss 3.951127503596536, accuracy 894\n",
      "Step 6001. For the last 1000 steps: average loss 2.2303354536766933, accuracy 933\n",
      "Step 7001. For the last 1000 steps: average loss 3.08811191838809, accuracy 910\n",
      "Step 8001. For the last 1000 steps: average loss 2.3585601279678117, accuracy 932\n",
      "Step 9001. For the last 1000 steps: average loss 1.271532498637427, accuracy 967\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "accuracy = 0\n",
    "for i, (image, label) in enumerate(zip(X_test[0:10000], Y_test[0:10000])):\n",
    "  if i % 1000 == 0: # Every 1000 examples\n",
    "    print(\"Step {}. For the last 1000 steps: average loss {}, accuracy {}\".format(i+1, loss/100, accuracy))\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "  loss_1, accuracy_1 = CNN_test(image, label, layers)\n",
    "  loss += loss_1\n",
    "  accuracy += accuracy_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f81b2245b40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_img = X_test[600]\n",
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The models prediction: 6\n"
     ]
    }
   ],
   "source": [
    "cnn_prediction = CNN_predict(test_img, layers)\n",
    "print(\"The models prediction: \" + str(cnn_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "convWeights = layers[0].getKernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.82310655 14.04900668 12.94255704]\n",
      " [14.36264239 17.23809272 14.7035848 ]\n",
      " [12.33595733 13.64436028 10.6148534 ]]\n"
     ]
    }
   ],
   "source": [
    "print(convWeights[0])\n",
    "np.savetxt('convWeights.txt', convWeights[0], fmt='%.8f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseWeights = layers[2].getParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.01053508e-03 -1.89200305e-03 -1.85310024e-03 ... -7.57742769e-03\n",
      "   7.12634503e-03 -7.26762816e-03]\n",
      " [-1.98507556e-03 -3.63137171e-03 -8.92301580e-03 ... -1.17171166e-02\n",
      "   7.92666174e-03  1.38552765e-02]\n",
      " [ 2.89037100e-04 -3.23959474e-03  1.53307673e-02 ... -4.88320152e-03\n",
      "  -8.61993495e-03 -3.30863416e-03]\n",
      " ...\n",
      " [-9.54646337e-02 -7.22884286e-02 -2.15960715e-01 ... -2.56481347e-01\n",
      "   1.27003348e-02  8.69323560e-01]\n",
      " [-1.52435365e-02 -1.74122316e-02 -7.60580747e-02 ... -1.21768545e-01\n",
      "  -2.88401365e-02  3.03309312e-01]\n",
      " [-2.73521682e-03 -1.17592438e-04  1.58185227e-02 ... -3.34695682e-03\n",
      "  -5.13511658e-03  9.89753110e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(denseWeights[0])\n",
    "np.savetxt('denseWeights.txt', denseWeights[0], fmt='%.8f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.10577872  1.07917191  0.21027185 -0.72485517 -0.03279999  2.473824\n",
      " -0.31919598  1.7240218  -2.66897325 -0.63568645]\n"
     ]
    }
   ],
   "source": [
    "print(denseWeights[1])\n",
    "np.savetxt('denseBias.txt', denseWeights[1], fmt='%.8f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
